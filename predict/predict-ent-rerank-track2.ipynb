{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12243516,"sourceType":"datasetVersion","datasetId":7714390}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install ftfy regex tqdm\n!pip -q install git+https://github.com/openai/CLIP.git\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download the private dataset zip\n# Unzip the dataset into a folder\n!gdown 1ttmGZdAZJ-4pA9Kz5SMfvff-G_-Xn0uM\n!unzip -q ./ENTRep_Private_Dataset_Update.zip -d ./ENTRep_Private_Dataset_update/\n\n# Download the CSV and related private image data\n!gdown 1d66ZMIef0HN8kTfsLzLKlgoAA5NXsI2I\n!unzip ./ENTRep_Track2_Private_Data.zip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# download the pretrained **Vector Field** model\n!gdown 1KgzoCoaDoFsLYReWHtX-2MdvflrxSGTQ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# use a trained **Rerank Model** to rerank results\n!gdown 1d-JhNGHCKGEIc_9vJYJtHwUPGeShJoOC\n!unzip -q Rerank_model.zip -d convnextbase-ensemble-metalearner","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport clip\nfrom PIL import Image\nimport torch.nn.functional as F\nimport random\nimport numpy as np\n\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nfrom PIL import Image\nimport json\nimport csv\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport timm\nimport pickle\nimport warnings\nimport torch.nn.init as init\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T08:21:49.456545Z","iopub.execute_input":"2025-07-02T08:21:49.456816Z","iopub.status.idle":"2025-07-02T08:21:53.914342Z","shell.execute_reply.started":"2025-07-02T08:21:49.456791Z","shell.execute_reply":"2025-07-02T08:21:53.913719Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"embed_dim = 512\nclass GaussianFourierProjection(nn.Module):\n    def __init__(self, embed_dim, scale=10.0):\n        super().__init__()\n        # Fixed random weights for projecting scalar t to higher frequency space\n        self.W = nn.Parameter(torch.randn(1, embed_dim // 2) * scale, requires_grad=False)\n\n    def forward(self, t):\n        # Ensure t has shape [B, 1]\n        if t.ndim == 1:\n            t = t.unsqueeze(-1)\n        proj = t * self.W  # Shape: [B, D/2]\n        # Return sinusoidal and cosinusoidal projection: [sin(tW), cos(tW)] → Shape: [B, D]\n        return torch.cat([torch.sin(proj), torch.cos(proj)], dim=-1)\n\nclass VectorField(nn.Module):\n    def __init__(self, dim, t_dim=32, hidden_dim=256, n_heads=4, dropout_prob=0.1):\n        super().__init__()\n        self.x_norm = nn.LayerNorm(dim)  # Normalize input embeddings\n        self.time_encoder = GaussianFourierProjection(t_dim)  # Time embedding module\n        self.dropout = nn.Dropout(dropout_prob)\n\n        # Create multiple independent heads (like a lightweight transformer block)\n        self.heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(dim + t_dim, hidden_dim),     # Project input + time\n                nn.LayerNorm(hidden_dim),\n                nn.SiLU(),                               # Activation: SiLU \n                nn.Dropout(dropout_prob),\n                nn.Linear(hidden_dim, dim)              # Back to original embedding dimension\n            ) for _ in range(n_heads)\n        ])\n\n        self.res_weight = nn.Parameter(torch.tensor(1.0))  # Learnable residual scaling\n        self.out_norm = nn.LayerNorm(dim)  # Final normalization (not applied directly here)\n        self.initialize_weights()\n\n    \n    def initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                # Use Kaiming initialization (good for ReLU/SiLU)\n                init.kaiming_uniform_(m.weight, nonlinearity='relu')\n                if m.bias is not None:\n                    init.zeros_(m.bias)\n\n\n    def forward(self, x, t):\n        # Handle scalar or 1D tensor time input → ensure shape [B, 1]\n        if not isinstance(t, torch.Tensor):\n            t = torch.full((x.shape[0], 1), t, device=x.device)\n        elif t.ndim == 0:\n            t = t.expand(x.shape[0], 1)\n        elif t.ndim == 1:\n            t = t.unsqueeze(-1)\n\n        x_normed = self.x_norm(x)                     # Normalize input\n        t_encoded = self.time_encoder(t.to(x.device)) # Encode time t\n        inp = torch.cat([x_normed, t_encoded], dim=-1)  # Concatenate along feature dim\n\n        # Pass through each head and average their outputs\n        head_outs = [head(inp) for head in self.heads]\n        out = torch.mean(torch.stack(head_outs), dim=0)\n\n        # Add residual connection scaled by learnable weight\n        return out + self.res_weight * x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T08:21:53.915359Z","iopub.execute_input":"2025-07-02T08:21:53.915747Z","iopub.status.idle":"2025-07-02T08:21:53.925859Z","shell.execute_reply.started":"2025-07-02T08:21:53.915718Z","shell.execute_reply":"2025-07-02T08:21:53.925097Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\nclass RerankModel:\n    def __init__(self, model_dir, device='cuda'):\n        self.model_dir = Path(model_dir)\n        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n\n        self.class_names = [\n            \"nose-right\", \"nose-left\", \"ear-right\",\n            \"ear-left\", \"vc-open\", \"vc-closed\", \"throat\"\n        ]\n        self.num_classes = len(self.class_names)\n\n        print(f\"Using device: {self.device}\")\n\n    def load_ensemble_models(self):\n        \"\"\"Load all models from the ensemble directory.\"\"\"\n        print(\"Loading ensemble models...\")\n\n        with open(self.model_dir / 'ensemble_info.pkl', 'rb') as f:\n            ensemble_info = pickle.load(f)\n\n        models = []\n        model_names = ensemble_info['models']\n        weights = ensemble_info['weights']\n\n        for i, model_name in enumerate(model_names):\n            print(f\"Loading model {i+1}/{len(model_names)}: {model_name}\")\n\n            if 'convnext' in model_name.lower():\n                base_name = \"convnext_base.fb_in22k_ft_in1k\"\n            elif 'efficientnet' in model_name.lower():\n                base_name = \"efficientnet_b4\"\n            else:\n                base_name = \"convnext_base.fb_in22k_ft_in1k\"\n\n            model = timm.create_model(base_name, pretrained=False, num_classes=self.num_classes)\n            state_dict = torch.load(self.model_dir / f\"ensemble_model_{i}.pt\", map_location=self.device)\n            model.load_state_dict(state_dict)\n            model.to(self.device)\n            model.eval()\n\n            models.append({'model': model, 'weight': weights[i], 'name': model_name})\n\n        print(f\"Loaded {len(models)} models successfully.\")\n        return models\n\n    def load_test_data(self, csv_path, img_dir):\n        \"\"\"Load test image paths from CSV file.\"\"\"\n        test_files = []\n        with open(csv_path, 'r') as f:\n            reader = csv.reader(f)\n            for row in reader:\n                if row:\n                    img_name = row[0].strip()\n                    img_path = Path(img_dir) / img_name\n                    if img_path.exists():\n                        test_files.append(str(img_path))\n                    else:\n                        print(f\"Warning: Image not found: {img_path}\")\n\n        print(f\"Loaded {len(test_files)} test images.\")\n        return test_files\n\n    def get_tta_transforms(self, img_size=224, n_aug=5):\n        \"\"\"Generate a list of transforms for Test Time Augmentation (TTA).\"\"\"\n        class ResizeOrPad:\n            def __init__(self, min_size):\n                self.min_size = min_size\n\n            def __call__(self, img):\n                w, h = img.size\n                if w < self.min_size or h < self.min_size:\n                    scale = self.min_size / min(w, h)\n                    new_w = int(w * scale)\n                    new_h = int(h * scale)\n                    return T.functional.resize(img, (new_h, new_w))\n                return img\n\n        transforms = [\n            T.Compose([\n                ResizeOrPad(img_size),\n                T.Resize((img_size, img_size)),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225])\n            ])\n        ]\n\n        import random\n        for i in range(n_aug - 1):\n            resize_delta = random.choice([-20, -10, 0, 10, 20])\n            target_size = max(img_size, img_size + resize_delta)\n\n            transforms.append(\n                T.Compose([\n                    ResizeOrPad(target_size + 20),\n                    T.Resize((target_size + 10, target_size + 10)),\n                    T.CenterCrop(img_size) if i % 2 == 0 else T.RandomCrop(img_size),\n                    T.RandomApply([\n                        T.ColorJitter(\n                            brightness=random.uniform(0.1, 0.2),\n                            contrast=random.uniform(0.1, 0.2),\n                            saturation=random.uniform(0.05, 0.15),\n                            hue=random.uniform(0.02, 0.05)\n                        )\n                    ], p=0.8),\n                    T.RandomChoice([\n                        T.GaussianBlur(3, sigma=(0.1, 0.5)),\n                        nn.Identity()\n                    ]),\n                    T.ToTensor(),\n                    T.Normalize(mean=[0.485, 0.456, 0.406],\n                                std=[0.229, 0.224, 0.225])\n                ])\n            )\n        return transforms\n\n    def predict_single_image_tta(self, model, img_path, n_aug=5):\n        \"\"\"Predict a single image using Test Time Augmentation (TTA).\"\"\"\n        image = Image.open(img_path).convert('RGB')\n        tta_transforms = self.get_tta_transforms(n_aug=n_aug)\n\n        aug_probs = []\n        with torch.no_grad():\n            for transform in tta_transforms:\n                img_tensor = transform(image).unsqueeze(0).to(self.device)\n                output = model(img_tensor)\n                probs = F.softmax(output, dim=1)\n                aug_probs.append(probs)\n\n        weights = torch.tensor([1.5] + [1.0] * (n_aug - 1)).to(self.device)\n        weights = weights / weights.sum()\n\n        final_probs = torch.zeros_like(aug_probs[0])\n        for i, probs in enumerate(aug_probs):\n            final_probs += probs * weights[i]\n\n        return final_probs.cpu().numpy().squeeze()\n\n    def ensemble_predict_batch(self, models, test_files, use_tta=True, batch_size=32):\n        \"\"\"Predict a batch of images by ensembling multiple models.\"\"\"\n        predictions = {}\n        for img_path in test_files:\n            img_name = Path(img_path).name\n            all_model_probs = []\n            model_weights = []\n\n            for model_info in models:\n                model = model_info['model']\n                weight = model_info['weight']\n\n                if use_tta:\n                    probs = self.predict_single_image_tta(model, img_path, n_aug=3)\n                else:\n                    transform = T.Compose([\n                        T.Resize((224, 224)),\n                        T.ToTensor(),\n                        T.Normalize(mean=[0.485, 0.456, 0.406],\n                                    std=[0.229, 0.224, 0.225])\n                    ])\n                    image = Image.open(img_path).convert('RGB')\n                    img_tensor = transform(image).unsqueeze(0).to(self.device)\n                    with torch.no_grad():\n                        output = model(img_tensor)\n                        probs = F.softmax(output, dim=1).cpu().numpy().squeeze()\n\n                all_model_probs.append(probs)\n                model_weights.append(weight ** 2)\n\n            model_weights = np.array(model_weights)\n            model_weights /= model_weights.sum()\n\n            final_probs = np.zeros_like(all_model_probs[0])\n            for i, probs in enumerate(all_model_probs):\n                final_probs += probs * model_weights[i]\n\n            pred_class = np.argmax(final_probs)\n            predictions[img_name] = int(pred_class)\n\n        return predictions\n\n    def predict_and_save(self, csv_path, img_dir, output_path, use_tta=True):\n        \"\"\"Run predictions and save the results to a JSON file.\"\"\"\n        models = self.load_ensemble_models()\n        test_files = self.load_test_data(csv_path, img_dir)\n\n        print(\"\\nStarting prediction...\")\n        predictions = self.ensemble_predict_batch(models, test_files, use_tta=use_tta)\n\n        print(f\"\\nSaving results to {output_path}\")\n        with open(output_path, 'w') as f:\n            json.dump(predictions, f, indent=2)\n\n        print(f\"Saved {len(predictions)} predictions\")\n\n        print(\"\\nSample predictions:\")\n        for i, (img_name, pred) in enumerate(list(predictions.items())[:5]):\n            print(f\"  {img_name}: {pred} ({self.class_names[pred]})\")\n\n        return predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T08:21:54.288523Z","iopub.execute_input":"2025-07-02T08:21:54.288811Z","iopub.status.idle":"2025-07-02T08:21:54.311023Z","shell.execute_reply.started":"2025-07-02T08:21:54.288788Z","shell.execute_reply":"2025-07-02T08:21:54.310402Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n#Function to preprocess and embed an image \ndef preprocess_and_embed(image_path):\n    try:\n        # Load and convert image to RGB\n        image = Image.open(image_path).convert(\"RGB\")\n    except Exception as e:\n        raise RuntimeError(f\"Image open failed: {e}\")\n\n    # Apply CLIP preprocessing and move to device\n    image = preprocess(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        # Generate image embedding using CLIP\n        emb = model.encode_image(image)\n\n        # Check that the embedding has expected shape [1, 512]\n        if emb.ndim != 2:\n            raise RuntimeError(f\"Unexpected embedding shape: {emb.shape}\")\n\n        # Normalize the embedding vector\n        emb = emb / emb.norm(dim=-1, keepdim=True)\n\n        # Ensure correct dtype before passing to vector field\n        emb = emb.to(next(vf.parameters()).dtype)\n\n        # Apply learned vector field at time t = 0.0\n        emb_vf = vf(emb, t=torch.tensor([[0.0]], device=emb.device)).squeeze(0).cpu()\n\n        # Ensure the output is a 1D vector\n        if emb_vf.ndim != 1:\n            raise RuntimeError(f\"VectorField output is not 1D: {emb_vf.shape}\")\n\n    return emb_vf  # Final transformed embedding\n    \ndef rerank_topk_by_class(candidate_names, candidate_scores, predictor, models, img_dir, bonus=0.01):\n    \"\"\"\n    Re-rank a list of top-k image candidates by promoting those that share the dominant class.\n\n    Args:\n        candidate_names (List[str]): Top-k image file names to be re-ranked.\n        candidate_scores (List[float]): Corresponding similarity scores.\n        predictor (RerankModel): Classifier wrapper to load and use ensemble models.\n        models (List): List of loaded classification models.\n        img_dir (str): Path to image directory.\n        bonus (float): Bonus score to add for images in the dominant class.\n\n    Returns:\n        reranked_names (List[str]): Candidate names sorted by adjusted score.\n    \"\"\"\n    name_to_class = {}   # Map image name → predicted class\n    class_votes = {}     # Count class frequency\n\n    # Predict class for each candidate\n    for img_name in candidate_names:\n        img_path = os.path.join(img_dir, img_name)\n        pred_class = predictor.ensemble_predict_batch(models, [img_path], use_tta=False)[img_name]\n        name_to_class[img_name] = pred_class\n        class_votes[pred_class] = class_votes.get(pred_class, 0) + 1\n\n    # Identify majority class\n    main_class = max(class_votes, key=class_votes.get)\n\n    # Apply bonus score for images in majority class\n    reranked = []\n    for name, score in zip(candidate_names, candidate_scores):\n        bonus_score = bonus if name_to_class[name] == main_class else 0.0\n        reranked.append((name, score + bonus_score))\n\n    # Sort descending by score\n    reranked = sorted(reranked, key=lambda x: -x[1])\n    reranked_names = [name for name, _ in reranked]\n    return reranked_names\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T08:22:12.409669Z","iopub.execute_input":"2025-07-02T08:22:12.410216Z","iopub.status.idle":"2025-07-02T08:22:12.416263Z","shell.execute_reply.started":"2025-07-02T08:22:12.410190Z","shell.execute_reply":"2025-07-02T08:22:12.415581Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## **Retrieval Pipeline Description**\n\nImage-to-image retrieval is performed in 4 main steps:\n\n1. **Image Loading**: Read all image file names from a CSV file.\n\n2. **Embedding Extraction**: Use CLIP + VectorField to compute embeddings for all images.\n\n3. **Similarity Search**: For each query image, retrieve top-5 similar images using cosine similarity.\n\n4. **Class-based Reranking**: Use an ensemble classifier to predict classes of the top-5 candidates. Images sharing the dominant class receive a score bonus (+0.01) before reranking.\n\nThe top-1 result after reranking is stored for each query image.\n","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# Load pre-trained CLIP model (ViT-B/32) for image embedding\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# Initialize the learned time-dependent vector field module\nvf = VectorField(embed_dim).to(device).float()\n\n# Load the pre-trained weights for the vector field\nvf.load_state_dict(torch.load(\"./vf_model.pth\", map_location=device))\nvf.eval()  # Set to evaluation mode\n\n# Initialize the reranking model used for predicting anatomical regions\npredictor = RerankModel(model_dir=\"./convnextbase-ensemble-metalearner\")\n\n# Load ensemble of classification models from the specified directory\nmodels = predictor.load_ensemble_models()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T08:22:14.672542Z","iopub.execute_input":"2025-07-02T08:22:14.673149Z","iopub.status.idle":"2025-07-02T08:22:30.145288Z","shell.execute_reply.started":"2025-07-02T08:22:14.673118Z","shell.execute_reply":"2025-07-02T08:22:30.144572Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading ensemble models...\nLoading model 1/6: convnext_base.fb_in22k_ft_in1k_full\nLoading model 2/6: convnext_base.fb_in22k_ft_in1k_fold3\nLoading model 3/6: convnext_base.fb_in22k_ft_in1k_fold1\nLoading model 4/6: convnext_base.fb_in22k_ft_in1k_fold4\nLoading model 5/6: convnext_base.fb_in22k_ft_in1k_fold5\nLoading model 6/6: convnext_base.fb_in22k_ft_in1k_fold2\nLoaded 6 models successfully.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Step 1: Read image names from CSV\nimage_dir = \"./ENTRep_Private_Dataset_update/imgs\"  # Path to directory containing test images\ncsv_path = \"i2i.csv\"  # CSV file with image names (1 name per line)\n\nwith open(csv_path, \"r\") as f:\n    reader = csv.reader(f)\n    image_list = [row[0].strip() for row in reader if row]  # List of image file names\n\n\n#Step 2: Compute embeddings for all images\n# Output: all_embeddings[img_name] = torch.Tensor of shape [D]\n\nall_embeddings = {}\n\nfor img_name in image_list:\n    img_path = os.path.join(image_dir, img_name)  # Full path to the image\n    try:\n        emb = preprocess_and_embed(img_path)  # Generate CLIP+VectorField embedding\n        all_embeddings[img_name] = emb\n       \n    except Exception as e:\n        print(f\"❌ Error with {img_name}: {e}\")\nprint(\" Embedded all images\")\n\n#Step 3: Filter valid embeddings and stack into a tensor\n# Output: embeddings: torch.Tensor [N, D], img_names: List[str]\nvalid_img_names = []\nvalid_embeddings = []\n\nfor name in all_embeddings:\n    emb = all_embeddings[name]\n    # Ensure embedding is a 1D torch.Tensor (e.g., shape = [512])\n    if isinstance(emb, torch.Tensor) and emb.ndim == 1:\n        valid_img_names.append(name)\n        valid_embeddings.append(emb)\n    else:\n        print(f\"⚠️ Invalid embedding: {name} → {type(emb)}, shape = {getattr(emb, 'shape', None)}\")\n\nif not valid_embeddings:\n    raise ValueError(\"❌ No valid embeddings found!\")\n\n# Stack embeddings into a tensor and normalize\nimg_names = valid_img_names\nembeddings = torch.stack(valid_embeddings)  # Shape: [N, D]\nembeddings = F.normalize(embeddings, dim=-1)  # Cosine-normalized embeddings\n\n\n# Step 4: Image Retrieval with Class-based Reranking\n# For each image, find top-5 most similar, then re-rank by majority class\nretrieval_results = {}\n\nfor i, query_name in enumerate(img_names):\n    query_emb = embeddings[i].unsqueeze(0)  # Shape: [1, D]\n    # Exclude current image to avoid self-matching\n    others = torch.cat([embeddings[:i], embeddings[i+1:]], dim=0)  # Shape: [N-1, D]\n\n    # Compute cosine similarities\n    sims = (others @ query_emb.T).squeeze()  # Shape: [N-1]\n\n    # Retrieve top-5 most similar images\n    topk = torch.topk(sims, k=5)\n    topk_indices = topk.indices.tolist()\n    topk_scores = topk.values.tolist()\n\n    # Map indices to actual image names (adjust index if skipped self)\n    candidate_names = []\n    candidate_scores = []\n    for j, idx in enumerate(topk_indices):\n        idx_adjusted = idx if idx < i else idx + 1\n        candidate_names.append(img_names[idx_adjusted])\n        candidate_scores.append(topk_scores[j])\n\n    # Re-rank candidates using class prediction\n    reranked = rerank_topk_by_class(\n        candidate_names, candidate_scores,\n        predictor, models, image_dir, bonus=0.01\n    )\n\n    # Save only the top-1 match\n    retrieval_results[query_name] = reranked[0]\nprint(\"Retrieval completed for all images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T08:22:36.514228Z","iopub.execute_input":"2025-07-02T08:22:36.514816Z","iopub.status.idle":"2025-07-02T08:26:34.437752Z","shell.execute_reply.started":"2025-07-02T08:22:36.514792Z","shell.execute_reply":"2025-07-02T08:26:34.437026Z"}},"outputs":[{"name":"stdout","text":" Embedded all images\nRetrieval completed for all images\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"output_json = \"rerank003.json\"\n# Save results\nwith open(output_json, \"w\") as f:\n    json.dump(retrieval_results, f, indent=2)\nprint(f\"Saved top-1 retrieval results with rerank to: {output_json}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T08:27:26.976679Z","iopub.execute_input":"2025-07-02T08:27:26.976983Z","iopub.status.idle":"2025-07-02T08:27:26.982727Z","shell.execute_reply.started":"2025-07-02T08:27:26.976962Z","shell.execute_reply":"2025-07-02T08:27:26.982153Z"}},"outputs":[{"name":"stdout","text":"Saved top-1 retrieval results with rerank to: rerank003.json\n","output_type":"stream"}],"execution_count":9}]}